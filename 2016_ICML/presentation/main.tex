\documentclass[11pt]{beamer}
%\documentclass[11pt,handout]{beamer}

\usepackage{multimedia}

%\usetheme[secheader]{Boadilla}
\usetheme{Boadilla}

%\setbeameroption{hide notes}
%\setbeameroption{show notes}
%\setbeameroption{show only notes}
%\setbeamertemplate{note page}[plain]

\usefonttheme{structuresmallcapsserif}
\setbeamerfont{frametitle}{size=\normalsize}
\setbeamertemplate{navigation symbols}{}

\AtBeginSection[]{}

\setbeamercovered{invisible}

% Custom packages
\usepackage{bibentry}
\usepackage{color}

\usepackage{animate}

% Customization
\newcommand{\footcite}[1]{$^[$\footnote{\begin{tiny}\bibentry{#1}\end{tiny}}$^]$}

\renewcommand{\emph}[1]{\textbf{#1}}

\newcommand{\taskparam}{{\color{red} \tau}}
\newcommand{\skillparam}{{\color{blue} \theta}}

%\newcommand{\newblock}{}

% Author, Title, etc.
\title[ICML 2016]{Minimum Regret Search for Single- and Multi-Task Optimization}
\author[Jan Hendrik Metzen]{Jan Hendrik Metzen}
\institute[]{University Bremen, Robotics Research Group \\ Robert Bosch GmbH, Corporate Research CR/AEY2}
\date[06/19/16]{ICML, June 19, 2016}

\begin{document}

\frame{\titlepage
       \bibliographystyle{abbrv}
       \nobibliography{../literature}}

\begin{frame}{Motivation: Bayesian optimization}


\end{frame}

\begin{frame}{Bayesian Optimization}
\textbf{Bayesian optimization}\footcite{shahriari_taking_2016}:
\begin{itemize}
\item black-box optimization problems: $\mathbf{x} = \arg\min_{\mathbf{x} \in \mathcal{X}} f(x)$ of some function $f: \mathcal{X} \to \mathbb{R}$ on some bounded set $\mathcal{X} \subset \mathbb{R}^D$.
\item \emph{probabilistic model} $p(f)$ for $f(\mathbf{x})$, typically a Gaussian process (GP)
\item based on GP posterior of first $n$ observations $\mathcal{D}_n=\{(\mathbf{x}_i, y_i)\}_{i=1}^n$ and an \emph{acquisition function}: decides on a query point $\mathbf{x}_{n+1}$ where $f$ will be evaluated next
\item recommends $\mathbf{\tilde x}_N$ after $N$ queries (optimum of GP or best query point)
\item objective: minimize \emph{simple regret} 
$R_f(\mathbf{\tilde  x}_N) = f(\mathbf{x}^\star) - f(\mathbf{\tilde  x}_N) = \max_{\mathbf{x}} f(\mathbf{x}) - f(\mathbf{\tilde x}_N)$
\end{itemize}
\end{frame}

\begin{frame}{Entropy Search}
\begin{itemize}
\item Let
   \begin{itemize}
   \item $p^\star(x \vert \mathcal{D}_n)$ denote the posterior distribution (after observing $\mathcal{D}_n$) of the unknown optimizer $\mathbf{x}^\star = \arg\max_{\mathbf{x} \in \mathcal{X}} f(\mathbf{x})$.
   \item $H(\mathbf{x}^\star \vert \mathcal{D}_n)$ denote the differential entropy of $p^\star(x \vert \mathcal{D}_n)$
   \end{itemize}
   \item Entropy Search (ES)\footcite{hennig_entropy_2012} is an information theoretic acquisition fct.:
$$a_{ES}(\mathbf{x}, \mathcal{D}_n) = H(\mathbf{x}^\star \vert \mathcal{D}_n) - \mathbb{E}_{y \vert \mathbf{x}, \mathcal{D}_n}   [H(\mathbf{x}^\star \vert \mathcal{D}_n \cup \{(\mathbf{x}, y))]$$
   \end{itemize}
   \pause

   \vspace*{-.25cm}
   \begin{figure}
   \centering
   \includegraphics[width=0.48\columnwidth]{../pics/regret_illustration_2}
   \visible<3->{
     \includegraphics[width=0.48\columnwidth]{../pics/regret_scatter}
   }
   \vspace*{-.25cm}
   \caption{(Left) GP posterior, probability of maximum $p^\star$, and expected regret ER. (Right) Scatter plot of  position of optimum and regret of $\mathbf{\tilde x} = 1.5$ versus optimum in sample functions.}
   \label{fig:MRS_illustration}
   \end{figure}
\end{frame}


\begin{frame}{Minimum Regret Search}

MRS: acquistion function based directly on external objective of minimizing simple regret
\begin{itemize}
 \item Expected simple regret: $\text{ER}(p)(\mathbf{x}) = \mathbb{E}_{p(f)}[R_f(\mathbf{x})]
= \mathbb{E}_{p(f)}[\max_{\mathbf{x}} f(\mathbf{x}) - f(\mathbf{
x})]$
 \item For fixed GP $p(f)$, $\arg\min_\mathbf{x} \text{ER}(p)(\mathbf{x})$ corresponds to the maximizer of the GP mean
 \item MRS aims at selecting query points s.t. ER is minimized also with respect to resulting $p(f)$
 \item Myopic choice: select next query point s.t. minimum ER is reduced the most (in expectation)
  $$a_{\text{MRS}^{\text{point}}}(\mathbf{x}^q)
  = \min_{\mathbf{\tilde x}}\text{ER}(p_n)(\mathbf{\tilde x})
   - \mathbb{E}_{y \vert p_n(f), \mathbf{x}^q}[\min_{\mathbf{\tilde x}}  \text{ER}(p^{[\mathbf{x}^q, y]}_n)(\mathbf{\tilde x})]$$
 \item MRS additionally also accounts for uncertainty in minimizer $\mathbf{\tilde x}$ of $\text{ER}$ 
  $$a_{\text{MRS}}(\mathbf{x}^q)
    = \mathbb{E}_{\mathbf{\tilde x} \sim p^\star_n}[\text{ER}(p_n)(\mathbf{\tilde x})] 
     - \mathbb{E}_{y \vert p_n(f), \mathbf{x}^q}[
      \mathbb{E}_{\mathbf{\tilde x} \sim p^\star_{\mathcal{D}_n \cup \{(\mathbf{x}^q, y)\}}}[
       \text{ER}(p^{[\mathbf{x}^q, y]}_n)(\mathbf{\tilde x})]]$$
\end{itemize}

\end{frame}

\begin{frame}{Minimum Regret Search Illustration}
\begin{figure}
\centering
\includegraphics[width=0.48\columnwidth]{../pics/acq_comparison}
\end{figure}

\begin{itemize}
\item MRS$^\text{point}$ tends to query in areas which are risky for current recommendation (large simple regret possible)
\item MRS is more smooth than MRS$^\text{point}$ since it accounts for uncertainty in recommendation
\end{itemize}

\end{frame}


\begin{frame}{Experimental Setup}
Synthetic Single-Task Benchmark:
\begin{itemize}
 \item Target functions sampled from a generative model on $\mathcal{X} = [0, 1]^2$ 
 \item In practice: 
   \begin{itemize} 
   \item sample $250$ pairs $(\mathbf{x}, f(\mathbf{x}))$ from function $f \sim GP(f)$
   \item fit another GP to these pairs
   \item use resulting posterior mean as target function
   \end{itemize}
 \item Gaussian noise with standard deviation $\sigma =10^{-3}$ is added to each observation
 \item Probabilistic surrogate model used in BO:\\ GP with isotropic RBF kernel of length scale $l = 0.1$ and unit signal variance
 \pause
 \item Generative model
    \begin{itemize} 
    \item without model mismatch: GP with isotropic RBF kernel ($l = 0.1$ and unit signal variance)
    \item with model mismatch:  GP with isotropic rational quadratic kernel ($l = 0.1$, $\alpha=1.0$ and unit signal variance)
    \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{Results: No model-mismatch}
\begin{figure}
\centering
\includegraphics[width=.7\columnwidth]{../pics/empirical_comparison} \\
\includegraphics[width=.7\columnwidth]{../pics/hist}
\end{figure}
\end{frame}


\begin{frame}{Results: }
\begin{figure}
\centering
\includegraphics[width=.8\textwidth]{../pics/es_analysis}
\label{fig:es_analysis}
\end{figure}

Acquisition functions on a target function at $N=100$ and 25 representer points; darker areas correspond to larger values. ES focuses on sampling in areas with high density of $p^\star$ (many representer points), while MRS focuses on unexplored areas that are populated by representer points (non-zero $p^\star$).

\end{frame}


\begin{frame}{Results: Model-mismatch}
\begin{figure}
\centering
\includegraphics[width=.7\textwidth]{../pics/empirical_comparison_mm} \\
\includegraphics[width=.7\textwidth]{../pics/hist_mm}
\end{figure}
\end{frame}


\begin{frame}{Outlook}
  
  \begin{itemize}
    \item Additional content of the paper:
    \begin{itemize}
      \item Multi-task Minimum regret search
      \item results on simulated robotic task
    \end{itemize}
    \pause
    \item Future work:
    \begin{itemize}
      \item treatment of GP hyperparameters
      \item more efficient approximation techniques for MRS
    \end{itemize}
    \pause
    \item Source code available at: \url{https://github.com/jmetzen/bayesian_optimization}
  \end{itemize}

  \vspace*{1.5cm}
  \pause
  \begin{center}
   Thank you for your attention! \\
   Do you have questions, comments, or ideas?
   \end{center}

\end{frame}

\end{document}
